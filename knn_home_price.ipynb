{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting home prices with the k-NN algorithm\n",
    "Given a record containing features of a house, we want to be able to predict its price. We use the dataset ''House Sales in King County, USA'', downloaded from kaggle. We want to predict a price of a home based on the homes that are closests to it, i.e. have similar properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data file [housing.csv](https://drive.google.com/file/d/1s_muxczF8K4qs5mIohZItlb0b6jA4Rhc/view?usp=sharing) to your local directory.<br>\n",
    "Update the variable `file_name` in the cell below to point to your local directory where you store the datasets for this course and then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"../data_sets/housing.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Predicting home price\n",
    "### 1.1. Features are numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following features:\n",
    "<ul>\n",
    "    <li>id - house identifier, numeric.</li>\n",
    "    <li>price - house price, numeric. <b>This is the target variable that we are trying to predict</b>.</li>\n",
    "    <li>bedrooms - no. of bedrooms, numeric.</li>\n",
    "    <li>bathrooms - no. of bathrooms, numeric.</li>\n",
    "    <li>sqft_living - square footage of the home, numeric.</li>\n",
    "    <li>sqft_lot - square footage of the lot, numeric.</li>\n",
    "    <li>floors - no.of floors, numeric.</li>\n",
    "    <li>waterfront - boolean (expressed as 0 or 1).</li>\n",
    "    <li>condition - the amount of wear-and-tear, numeric (from 0 to 5).</li>\n",
    "    <li>sqft_above - square footage of house apart from basement, numeric.</li>\n",
    "    <li>sqft_basement - square footage of the basement, numeric.</li>\n",
    "    <li>age - number of years since year built to year sold, numeric.</li>\n",
    "</ul>\n",
    "<br>\n",
    "Read the file into pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# this creates a pandas.DataFrame\n",
    "data = pd.read_csv(file_name, index_col='id')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there any correlation between features?\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "corr = data.corr()\n",
    "sns.heatmap(corr, center=0, annot=True, linewidths=.1, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. The target is numeric\n",
    "We want to build the model that *predicts* house prices: we look into `sklearn.neighbors` [library](https://scikit-learn.org/stable/modules/neighbors.html). There are several modules in this library. Which one do you think we need to use in order to predict prices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into features and target variable\n",
    "X = data.drop(columns=['price' ])\n",
    "Y = data['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to split the dataset into train and test parts we can use `train_test_split` method from `sklearn.model_selection` module. We set the test set size to be 20% of the entire dataset. This way of splitting the data into train and test sets is called **holdout estimation**: we are holding out part of the data to see how the predictor performs on data that it has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split dataset into train and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now buid our model. What do you think happens when we call `knn.fit`? What operations are performed by the algorithm? \n",
    "\n",
    "Explore all different algorithms [here](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms).\n",
    "\n",
    "# Task 1. Question\n",
    "\n",
    "Does the the use of a different algorithm make the model itself different? What is the difference between the algorithms?\n",
    "\n",
    "What does the `weight` parameter specify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(n_neighbors=3)\n",
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set score\n",
    "knn.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set score\n",
    "knn.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried all the imaginable improvements and was unable to get the test accuracy above $0.60$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Predicting Home Category\n",
    "It does not seem that we can reliably predict the numeric home price using the data and the `KNeighborsRegressor`.\n",
    "\n",
    "Instead we are going to use `KNeighborsClassifier` to predict a class label of each home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Converting target variable into a class label\n",
    "Convert the numeric price attribute into the binary class as follows: \"price above mean\" (class 1) and \"price below mean\" (class 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting target attribute to a class label\n",
    "# <Your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that both classes are represented properly, find out how many total houses are above mean and how many are below mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many houses belong to class 0 and to class 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are my results for comparison:\n",
    "\n",
    "mean price: 540182.1587933188\n",
    "\n",
    "Total: 21613\n",
    "\n",
    "Below: 13694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Baseline experiment\n",
    "\n",
    "Repeat the same steps as in 1.2. using transformed dataset and `KNeighborsClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we repeat the same steps but with a categorical class label\n",
    "# split data into features and target variable\n",
    "\n",
    "\n",
    "# split dataset into train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a clasifier instead of regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline train score:\", knn.score(X_train, Y_train))\n",
    "print(\"Baseline test score:\", knn.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some results for comparison:\n",
    "\n",
    "Baseline train score: 0.8657027183342972\n",
    "\n",
    "Baseline test score: 0.7594263243118204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Normalizing numeric attributes\n",
    "The k-Nearest Neighbor classifier uses distance to find $K$ nearest neighbors. Distance metric is very sensitive to the scale of numeric attributes. For example, the *sqft_living* is represented as three- to four-digit numbers, and the numer of *bedrooms* is in single digits. Thus the distance across *sqft_living* would dominate the distance across the *bedrooms* dimension. To avoid this scale-related bias, we need to project all numeric values into interval from 0 to 1. \n",
    "\n",
    "Perform data normalization in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing numeric fields\n",
    "# You can write a simple loop over all columns in data\n",
    "# You can use pandas or numpy\n",
    "# Use np.min and np.max\n",
    "# Alternatively you can use sklearn.preprocessing.MinMaxScaler()\n",
    "# <Your code here>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same experiment as in Task 3, but with normalized data. Did your model improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we repeat the same steps but with normalized data\n",
    "# split data into features and target variable\n",
    "\n",
    "#split dataset into train and test data\n",
    "\n",
    "# Build a classifier\n",
    "\n",
    "\n",
    "print(\"Normalized train score:\", knn.score(X_train, Y_train))\n",
    "print(\"Normalized test score:\", knn.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are my results:\n",
    "\n",
    "Normalized train score: 0.8728744939271255\n",
    "\n",
    "Normalized test score: 0.7777006708304418"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5. Adding expert knowledge\n",
    "Now that we removed the scale bias of each feature by projecting all of them into the same interval $[0,1]$, we want to introduce some bias based on the expert knowledge. \n",
    "\n",
    "Look at the correlation map. Which attributes are highly correlated with price? How can we make them contribute more to the overall distance between the houses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase influence of distances across important attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same experiment but with added bias. Did you get better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we repeat the same steps but with added bias\n",
    "# split data into features and target variable\n",
    "\n",
    "#split dataset into train and test data\n",
    "\n",
    "\n",
    "# Build the model\n",
    "\n",
    "print(\"Expert train score:\", knn.score(X_train, Y_train))\n",
    "print(\"Expert test score:\", knn.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got more than 78% accuracy for the testing score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6. Neighbor voting\n",
    "Look closely at the `weights` parameter of the `KNeighborsClassifier`. How can we make the nearer neighbors contribute more to the decision about the class? \n",
    "\n",
    "Run the same experiment as in 2.4 but with weighted distance from the neighbors. Did you get better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Weighted distance train score:\", knn.score(X_train, Y_train))\n",
    "print(\"Weighted distance score:\", knn.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7. More reliable score with cross-validation\n",
    "\n",
    "To produce a more reliable estimation of the model performance we are going to use **cross-validation** instead of holdout estimation. \n",
    "\n",
    "Cross-validation is when the dataset is randomly split up into $m$ groups (called $folds$). One of the groups is used as the test set and the rest are used as the training set. The model is trained on the training set and scored on the test set. Then the process is repeated until each unique group has been used as the test set. For example, for 5-fold cross validation, the dataset would be split into 5 groups, and the model would be trained and tested 5 separate times so each group would get a chance to be the test set. \n",
    "\n",
    "Cross-validation is more reliable than the holdout method because the holdout method score is dependent on how the data is split into train and test sets. Cross-validation gives the model an opportunity to test on multiple splits so we can get a better idea on how the model will perform on unseen data.\n",
    "\n",
    "Repeat the experiment from 2.5, but using the entire sets X and Y, and 10-fold cross-validation. You can use `sklearn.model_selection.cross_val_score` to compute the scores for each fold: [link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html).\n",
    "\n",
    "You should print all the scores and report the mean. Notice how the score for each fold is slightly different. The cross-validation helps to make model validation more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#create a new KNN model\n",
    "\n",
    "\n",
    "# train model with cv of 10 \n",
    "cv_scores = cross_val_score(knn_cv, X, Y, cv=10)\n",
    "\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print('cv_scores mean:{}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8. Finding the best value of $K$\n",
    "As explained in the lecture, the best value of $K$ can be found using cross-validation. If $K$ is small, then it might be not enough information from the neighbors to correctly predict the target variable. If the number of neighbors is too big, then the prediction will incorporate noise (overfitting).\n",
    "\n",
    "We are going to run our classification with different values of $K$, to determine the best value that produces the highest score for the test data. \n",
    "\n",
    "Conduct a series of experiments varying $K$ from 1 to 35, and for each experiment perform 20-fold cross-validation. For this we can use `sklearn.model_selection.GridSearchCV`: [link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). \n",
    "\n",
    "In a nutshell, you create a new knn classifier, without specifying the value of $K$. You also create a parameter `grid_dictionary`, where the key is `n_neighbors` and the value is `np.range(1,35)`.\n",
    "Then you call the `GridSearchCV` passing as parameters the knn classifier, parameter grid, and `cv`-the number of folds for cross-validation. \n",
    "\n",
    "Finally, you fit the data, and wait until all the experiments are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#create new a knn model\n",
    "\n",
    "\n",
    "#create a dictionary of all values we want to test for n_neighbors\n",
    "\n",
    "\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "\n",
    "\n",
    "#fit model to data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best value of $K$? We can find out by looking into `best_params_` field of the `GridSearchCV` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check top-performing n_neighbors value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9. Final experiment\n",
    "Repeat the final cross-validation experiment with the best value of $K$ and compute the mean of the cross-validation score for our final model (cv=20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new KNN model\n",
    "\n",
    "\n",
    "# train model with cv of 20 \n",
    "\n",
    "\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print('cv_scores mean:{}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you performed all the improvements correctly, the mean cross-validation score should be at least $0.80$. This is already a somewhat useful model. Given a new house, you can feed its features into the model, and find out if it is a cheap or an expensive house - i.e. if it is priced above the mean price in the current area or below it.\n",
    "\n",
    "Of course you are welcome to improve the model even further for additional bonus points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10. Predicting class of two new houses\n",
    "We have two new houses for sale. The data about them is in file two_houses.csv, included in this repository.\n",
    "\n",
    "We can build the model using the entire dataset (X, Y).\n",
    "\n",
    "We want to predict the class label of these two houses. \n",
    "\n",
    "Do not forget to perform the same transformations on the new data as you performed on the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_full = KNeighborsClassifier(n_neighbors=31, weights='distance')\n",
    "knn_full.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = \"two_houses.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-data transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = knn_full.predict(x)\n",
    "print(\"Predicted price:\",y_predicted )\n",
    "print(\"Actual price:\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the KNN lab.\n",
    "\n",
    "Copyright &copy; 2022 Marina Barsky. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
